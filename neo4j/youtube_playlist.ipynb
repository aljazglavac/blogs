{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d43937-31c3-4815-845f-304b09e66e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain neo4j openai tiktoken pytube youtube_transcript_api env langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca051a-81ea-4245-badb-8fb4f97b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pytube import Playlist\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_message_histories.neo4j import Neo4jChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0125-e8e4-453a-84bb-2121d7463e65",
   "metadata": {},
   "source": [
    "## Using LangChain in combination with Neo4j to process YouTube playlists and perform Q&A flow\n",
    "\n",
    "### Motivation\n",
    "In a world of lengthy YouTube playlists, traditional learning can feel time-consuming and dull. Our motivation is to transform this process by making it dynamic and engaging. Rather than passively consuming content, I believe that sparking conversations can make learning more enjoyable and efficient.\n",
    "\n",
    "### Goal\n",
    "Our goal is to revolutionise how people interact with YouTube playlists. Users will actively engage in dynamic conversations inspired by the playlist content. We'll extract valuable information from video captions, process it, and integrate it into the Neo4j vector database. The conversational chain serving as a guide that leads users through an dialogue rooted in playlist content. My mission is to provide an interactive and personalised educational dialog, where users actively shape their learning journey.\n",
    "\n",
    "#### Technologies used\n",
    "Embarking on the exciting journey of conversational AI requires a firm grasp of the technological foundations, that meet the needs of our mission. For our purpose we use synergy of two cutting-edge technologies: LangChain, an open-source framework simplifying the orchestration of Large Language Models (LLMs), and Neo4j, a robust graph database made for optimal node and relationship traversal.\n",
    "\n",
    "LangChain, serving as the backbone in our quest for seamless interaction with LLMs. Its open-source nature enables developers to easily create and use the capabilities of these expansive language models. In our demo application, LangChain acts as the provider of an interface and construction of a conversational chain.\n",
    "\n",
    "At the heart of this interaction lies Neo4j, a graph database designed to unravel the complexities of interconnected nodes and relationships.\n",
    "\n",
    "Picture this: a user initiates the conversation with a query, setting in motion a captivating exchange with our Large Language Model. The magic happens as the vector representation of the user's input becomes a beacon for exploration within the Neo4j graph database. The result? A seamless fusion of structured knowledge and natural language understanding, culminating in a response that is not just accurate but deeply connected to the context of the user's inquiry.\n",
    "\n",
    "Recognising the importance of user experience, we introduce a conversational memory chain. Imagine a conversation where every question asked and every answer given becomes part of an evolving dialogue. This approach ensures that the interaction remains clear and coherent. By feeding all past questions and answers into the conversational memory chain alongside the latest query, we create a continuous narrative thread. The result? A more engaging, relevant, and user-centric conversation that evolves intelligently with each interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645380a3-77c9-46df-bb64-cedaa16b9503",
   "metadata": {},
   "source": [
    "### What will I cover in this tutorial\n",
    "1. Processing of YouTube playlists; reading captions\n",
    "2. Splitting each video captions into documents\n",
    "3. Feeding documents into Neo4j database\n",
    "4. Constructing conversational retrieval chain\n",
    "5. Performing queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df455324-5fb4-4e86-9391-1bbd308bb10c",
   "metadata": {},
   "source": [
    "### Processing of YouTube playlists\n",
    "Use `Playlist` package to retrieve all video IDs that are inside the given playlist. For every video, using `YouTubeLoader`, extract caption documents. Feed each document into text splitter. It is important to clear and preprocess the data before feeding it into text splitters. In our case, we ensured that we only considered English captions. The size of each chunk varies and should be set based on the nature of the documents. Smaller chunks, up to 256 tokens, capture information more granularly. Larger chunks provide our LLM with more context based on the information within each document. In our case, I decided to use a chunk size of 512. This decision was made because context is more imporant so we ensure contextual connection over multiple videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad79d5-471d-4b2d-94dd-16bae0323ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos from the playlist\n",
    "playlist_url = \"https://www.youtube.com/watch?v=1CqZo7nP8yQ&list=PL9Hl4pk2FsvUu4hzyhWed8Avu5nSUXYrb\"\n",
    "playlist = Playlist(playlist_url)\n",
    "video_ids = [_v.split('v=')[-1] for _v in playlist.video_urls]\n",
    "print(f\"Processing {len(video_ids)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f35ac-3381-4634-a93c-1e6e151cab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read their captions and process it into documents with above defined text splitter\n",
    "documents = []\n",
    "for video_id in video_ids:\n",
    "    try:\n",
    "      loader = YoutubeLoader(video_id=video_id)\n",
    "      documents.append(loader.load()[0])\n",
    "    except: # if there are no english captions\n",
    "      pass\n",
    "print(f\"Read captions for {len(documents)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c02c4b-ce03-4bb1-8f54-befc15a42570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text splitter with chunk size 512 (https://www.pinecone.io/learn/chunking-strategies/)\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=20)\n",
    "# Split documents\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "print(f\"{len(splitted_documents)} documents ready to be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddca89-b674-4098-878f-1f38735ddbb0",
   "metadata": {},
   "source": [
    "### Feeding documents into Neo4j database\n",
    "As mentioned earlier, all the documents will be stored inside the Neo4j database. In return, we will obtain a vector index that will later be utilized in conjunction with [LangChain](https://www.langchain.com/). Creating a [Neo4j](https://neo4j.com/) database is fairly straightforward and can be done without any additional knowledge of how the database operates and functions. Since we have already prepared all our documents and split them, we used the `from_documents` function, which accepts a `List[Document]`. To simplify this process even further, we could also use the `from_texts` function. However, in this case, we would lose control over documents. Therefore, I believe that `from_texts` should only be used when we quickly want to demonstrate an application. \n",
    "\n",
    "Setting `search_type` to `hybrid` will allow us to search over keywords and vectors. Hybrid search combines results from both [full text search](https://neo4j.com/docs/cypher-manual/current/indexes-for-full-text-search/) and vector queries which use different functions such as [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/). To merge the results, a [Reciprocal Rank Fusion (RRF)](https://safjan.com/Rank-fusion-algorithms-from-simple-to-advanced/#:~:text=The%20Reciprocal%20Rank%20Fusion%20(RRF,into%20a%20unified%20result%20set.) algorithm is used. Response at the end provides only one result set, which is determined by RRF algorithm. This combination of vector search with traditional search methods allows for more nuanced and contextually relevant search results, improving the accuracy and depth of insights. This approach is particularly useful in applications such as ours where we have new answer every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59ea58-9acb-47c2-9585-a7becf25f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env variables\n",
    "os.environ['OPENAI_API_KEY'] = \"OPENAI_API_KEY\"\n",
    "os.environ['NEO4J_URI'] = \"NEO4J_URI\"\n",
    "os.environ['NEO4J_USERNAME'] = \"NEO4J_USERNAME\"\n",
    "os.environ['NEO4J_PASSWORD'] = \"NEO4J_PASSWORD\"\n",
    "\n",
    "# Contruct vector\n",
    "neo4j_vector = Neo4jVector.from_documents(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    documents=splitted_documents,\n",
    "    url=os.environ['NEO4J_URI'],\n",
    "    username=os.environ['NEO4J_USERNAME'],\n",
    "    password=os.environ['NEO4J_PASSWORD'],\n",
    "    search_type=\"hybrid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816cdb1-2a5c-473a-b8ab-e8800cc59c10",
   "metadata": {},
   "source": [
    "![Graph1](youtube_playlist_1.png \"Graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb697b41-0b97-483d-86f8-b0858ccaef06",
   "metadata": {},
   "source": [
    "### Constructing conversational retrieval chain\n",
    "A conversational chain will be used to facilitate the Q&A flow. Setting `k` to `3` signals our retrieval chain to retain the last 3 messages in memory. These three messages will be passed to the LLM while performing queries. Adjusting this value will provide the LLM with more context during the Q&A flow. For retrieval, we will use the Neo4j vector instance generated earlier. Additionally, we set the maximum tokens (`max_tokens_limit`) to ensure that we stay within the specified limit.\n",
    "\n",
    "Understanding conversational retrieval chain is much easier if we split the process of asking a question and getting back the answer into three parts:\n",
    "1. Use the chat history and new question to create a \"standalone question\". This is done so that this question can be passed into the retrieval step to fetch relevant documents. If only the new question was passed in, then relevant context may be lacking. If the whole conversation was passed into retrieval, there may be unnecessary information there that would distract from retrieval.\n",
    "2. This new question is passed to the retriever and relevant documents are returned.\n",
    "3. The retrieved documents are passed to an LLM along with either the new question (default behavior) or the original question and chat history to generate a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab8d1d-063c-4485-b1ad-dbeae0aa8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Q&A object\n",
    "chat_mem_history = Neo4jChatMessageHistory(session_id=\"1\")\n",
    "mem = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\", \n",
    "    chat_memory=chat_mem_history, \n",
    "    return_messages=True\n",
    ")\n",
    "q = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.2),\n",
    "    memory=mem,\n",
    "    retriever=neo4j_vector.as_retriever(),\n",
    "    verbose=False,\n",
    "    max_tokens_limit=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ceb83d-0bd5-4571-b452-9e5999311aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Q&A flow - first question\n",
    "response = q.run('What can you tell me about the GenAI stack?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd345-a778-4196-99bf-641f8a87d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow up question that requires previous answers (memory)\n",
    "response = q.run('Who talked about it?')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033df70b-ebfc-4ceb-a0ac-a63a7fd2beed",
   "metadata": {},
   "source": [
    "### Graph representation\n",
    "Neo4j Aura database offers us [workspace](https://neo4j.com/docs/browser-manual/current/deployment-modes/neo4j-aura/), where we can run cypher queries and have a graphical presentation of graph that is being constructed during our interactions. To showcase how this conversation history chain is presented, we can take a look at the following graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34c191-15fb-43c2-8ca3-bf91abe23f43",
   "metadata": {},
   "source": [
    "![Graph3](youtube_playlist_3.png \"Graph3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a0b0b-92fc-4a89-a789-b0f07fbcec81",
   "metadata": {},
   "source": [
    "The above graph was displayed by executing the following cypher query:\n",
    "```\n",
    "MATCH p=(n:Session {id: \"1\"})-[:LAST_MESSAGE]->()<-[:NEXT*0..3]-() \n",
    "RETURN p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a37de-be9f-49d0-b97b-6294716e54fc",
   "metadata": {},
   "source": [
    "Chat history is represented as two types of nodes; an _ai_ and _human_ node. Each of this nodes are connected with a _NEXT_ connection that forms a chain relationship. Each node has its id and content. There is also our main session node, which has _LAST_MESSAGE_ connection to the last message that was returned by the ai. By following the chain, we can see how questions were asked by the _human_ and how responses were returned by the _ai_. The above graph was constructed when we ran two questions from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec064fce-c181-40d3-a8cb-5aeba9e81044",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "In summary, our mission involved transforming YouTube learning by fostering dynamic conversations. We processed YouTube playlists using the Playlist package to extract video IDs and obtained caption documents through YouTubeLoader. Data preprocessing ensured consideration of only English captions, and text splitters handled document chunks. With a chunk size of 512, we prioritized context, crucial for maintaining connections across multiple videos.\n",
    "\n",
    "To facilitate a Q&A flow, a conversational chain with a retrieval chain (k set to 3) was employed. This retention of the last 3 messages aided the Language Model (LLM) in contextual understanding during queries. Retrieval leveraged a Neo4j vector instance generated earlier.\n",
    "\n",
    "The conversational retrieval chain involved creating a \"standalone question\" from the chat history and new question. This question was then passed to the retriever, which returned relevant documents. Finally, the LLM, given the retrieved documents and either the new or original question with chat history, generated a comprehensive response.\n",
    "\n",
    "_Disclaimer: This article was written with the help of ChatGPT._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
