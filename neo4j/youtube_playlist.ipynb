{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d43937-31c3-4815-845f-304b09e66e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain neo4j openai tiktoken pytube youtube_transcript_api env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deca051a-81ea-4245-badb-8fb4f97b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pytube import Playlist\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0125-e8e4-453a-84bb-2121d7463e65",
   "metadata": {},
   "source": [
    "## Using LangChain in combination with Neo4j to process YouTube playlists and perform Q&A flow\n",
    "\n",
    "### Motivation\n",
    "Listening or long playlists on YouTube can be time consuming and sometimes even boring. But having a converstaion regarding it is much more engaging and can produce far better results when it comes to the amount of new knowledge that we can take in. \n",
    "\n",
    "### Goal\n",
    "For a givven YouTube playlist read and proces captions. Feed them into Neo4j vector database and construct conversational chain that will guide users over Q&A flow.\n",
    "\n",
    "#### Technologies used\n",
    "We can not start without first understanding what are the technologies that we will be using. LangChain is an open-source framework that simplified the creation and usage of large language models (LLMs). In our case it is used as provider of interface that allows us to construct conversational chain which will use Neo4j vector store as source of trought. Neo4j is a graph database that was developed with intention of optimal traversal of nodes and relationships. With this two technologies we can performa a general pipeline, where users will ask a question which will be sent to the LLM. Vector representation of user input will be used to do a search inside graph database and the response is fed back to the LLM. Since we want to also ensure that users will have a good user experience, conversational memory chain will be added to the above described pipeline. This concept will allow us to feed all of the previous questions and answers parallel with newly asked question to the LLM. By doing so interaction will be clearer and response will have a greater relevance to the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ad79d5-471d-4b2d-94dd-16bae0323ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos from the playlist\n",
    "playlist_url = \"https://www.youtube.com/watch?v=1CqZo7nP8yQ&list=PL9Hl4pk2FsvUu4hzyhWed8Avu5nSUXYrb\"\n",
    "playlist = Playlist(playlist_url)\n",
    "video_ids = [_v.split('v=')[-1] for _v in playlist.video_urls]\n",
    "print(f\"Processing {len(videos)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6f2bcb-9c1f-42ee-b6e6-a46d7f65bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text splitter with chunk size 512 (https://www.pinecone.io/learn/chunking-strategies/)\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec7ebe4f-fbce-451b-ac7a-515e566f8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup username, passwords and api keys\n",
    "# from env import setup_env\n",
    "# setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a0f35ac-3381-4634-a93c-1e6e151cab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read their captions and process it into documents with above defined text splitter\n",
    "documents = []\n",
    "for video_id in video_ids:\n",
    "    try:\n",
    "      loader = YoutubeLoader(video_id=video_id)\n",
    "      documents.append(loader.load()[0])\n",
    "    except: # if there are no english captions\n",
    "      pass\n",
    "print(f\"Read captions for {len(documents)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2c02c4b-ce03-4bb1-8f54-befc15a42570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"I hope the toast is done you may have seen this video of an Olympic track cyclist powering a toaster and today the three of us are going to see just what that feels like 700 WTS for over a minute we've got Dave how much cycling experience do you have uh a couple Pelon rides I'm like a average cyclist and we have EJ over here how would you describe yourself I would say a racer I trained quite a bit he's being modest and how do you describe yourself don't bite drop it all right I have no idea what this going to feel like here we go now you may be wondering how can a stick figure like me be putting out the same amount of power as this Quadzilla the answer is urg mode his bike and mine are currently programmed to lock in at 700 WT give or take the exact amount of energy needed to power a toaster so it doesn't matter if we pedal faster or slower the resistance will automatically adjust to maintain 700 Watts which is lucky for me because I looked up his max Sprint an astronomical 2600 WTS or 3.6 horsepower he still going still going you you gave up at the end I think I feel good about that I don't know the hard thing with these efforts is like it does always feel like you could give a little bit more but in the moment it's like you can't profound I'm very self-aware how I'm going to probably perform today one thing I'm not going to wear those stupid shorts that all you bikers wear is it supposed to like hurt in my NE region all right so we're going to get Dave on a little more comfortable setup he's closer to EJ's side so we're going to put EJ's bike on um and then he and EJ will both ride it doesn't matter that it's a different bike because all the power comes through this thing this is the Wahoo trainer that's what connects to the iPad and sets the effort at 700 WTS so doesn't matter what bike it is this little bit is going to make sure you feel pain are you ready what what do you need uh something to protect my balls okay maybe the shoes would help too [Music] oh no okay okay here we go see up there yeah right got this like I'm traveling into [Music] hell jeez what the heck oh I wasn't expecting that so to give you context that's just under one horsepower that gives me no [Laughter] context oh my goodness before all of this we watched the original video together and decided we had to compare our thigh sizes to his 74 cm tree trunks okay hey 55 all right DJ I give me here we go 56 all right Dave go ahead measure your can you do it for and we'll record oh wow pretty pretty big thigh thanks all right what do you got we're 56 all right you guys are go die Brothers we're twins wow die Bros all right bud you go time to measure your thigh wow 16 wow wow okay this is this is my this is my Ste right here from the pros closet it's a large 2021 giant TCR Pro disc Advanced 2 I think that was that was come on first s that wasn't too bad now if any one of us has a chance of matching the Olympians time it's EJ his max Sprint may not be 2600 Watts but he can still put out a ton of power and more importantly he can keep it going for a long long time this is him riding up brri one of the steepest streets in San Francisco and look at that power you rate yesterday's ride oh I I like the ride itself 10 out of 10 super fun effort level it was pretty hard like there were definitely points where I was like I'm ready to be off the bike especially when we were up in the wind four more miles to the very top but it just got so windy it was uh it was more of a mental battle my legs feel okay today I have to tell you guys this morning he sent me a text he's like I'm ready to go with this whoop score it was like 70% and I was like wheezing in my bed trying to get up so if you want to see the video of our ride check out EJ's Channel he's going to make a vlog all about it sweet thank you you're welcome you can pay me the 20 bucks [Music] later yeah yeah dude you're almost there you got it you got it down up you got [Music] it got it you got it you got it you beat it keep going holy C you you beat him by like five or sixs at least god dude I hope the toast is done nobody can believe how much work at thank you my thigh size means nothing if you guys want to see Dave in another video and have him do this again make sure to comment down below so we can uh guilt him into it don't don't please no\", metadata={'source': 'c_pZmtCxb7Q'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split documents\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "print(f\"{len(splitted_documents} documents ready to be processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a59ea58-9acb-47c2-9585-a7becf25f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct vector\n",
    "neo4j_vector = Neo4jVector.from_documents(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    documents=splitted_documents,\n",
    "    url=os.environ['NEO4J_URI'],\n",
    "    username=os.environ['NEO4J_USERNAME'],\n",
    "    password=os.environ['NEO4J_PASSWORD'],\n",
    "    search_type=\"hybrid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ab8d1d-063c-4485-b1ad-dbeae0aa8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Q&A object\n",
    "chat_mem_history = ChatMessageHistory(session_id=\"1\")\n",
    "mem = ConversationBufferWindowMemory(k=3, memory_key=\"chat_history\", chat_memory=chat_mem_history, return_messages=True)\n",
    "q = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.2),\n",
    "    memory=mem,\n",
    "    retriever=neo4j_vector.as_retriever(),\n",
    "    verbose=True,\n",
    "    max_tokens_limit=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb73bdf3-15d2-4fa0-9dff-8b85ba508cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How much wantts does olimpic cyclists make during the sprint?',\n",
       " 'chat_history': [HumanMessage(content='How are you?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"As an AI, I don't have feelings, so I don't experience emotions like humans do. But thank you for asking! How can I assist you today?\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the video about?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"The content of the video is a group of individuals attempting to replicate the feat of an Olympic track cyclist who powered a toaster by generating 700 watts of power for over a minute. They discuss their cycling experience and then proceed to ride stationary bikes programmed to lock in at 700 watts. They share their experiences and compare their thigh sizes to the cyclist in the original video. They also mention that one of them, EJ, can generate a significant amount of power and has the best chance of matching the Olympian's time. The video concludes with a request for viewers to comment if they want to see one of the participants, Dave, do the challenge again in a future video.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the video about?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have access to the video content.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='how much watts is required to make a toast?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The video mentions that the exact amount of energy needed to power a toaster is around 700 watts.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How much wantts does olimpic cyclists make during the sprint?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The video mentions that the Olympic cyclist in question has a max sprint power of 2600 Watts or 3.6 horsepower.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Which type types of cyclists are in the video?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have access to the content of a specific video.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How much wantts does olimpic cyclists make during the sprint?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The maximum sprint power of Olympic cyclists mentioned in the context is 2600 watts.', additional_kwargs={}, example=False)],\n",
       " 'answer': 'The maximum sprint power of Olympic cyclists mentioned in the context is 2600 watts.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Q&A flow - first question\n",
    "response = q.run('What can you tell me about the GenAI stack?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd345-a778-4196-99bf-641f8a87d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow up question that requires previous answers (memory)\n",
    "response = q.run('Who talked about it?')\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
