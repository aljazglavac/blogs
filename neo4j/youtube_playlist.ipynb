{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d43937-31c3-4815-845f-304b09e66e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.302)\n",
      "Requirement already satisfied: neo4j in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (5.12.0)\n",
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: pytube in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (15.0.0)\n",
      "Requirement already satisfied: youtube_transcript_api in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.6.1)\n",
      "Collecting env\n",
      "  Downloading env-0.1.0.tar.gz (1.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.0.41)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.8.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.4.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from neo4j) (2023.3.post1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Building wheels for collected packages: env\n",
      "  Building wheel for env (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for env: filename=env-0.1.0-py3-none-any.whl size=2014 sha256=ccfca324ddf909cc5a8f08f7536c1ab42d65e5943118a5eb691ffee02cb59390\n",
      "  Stored in directory: /Users/aljazglavac/Library/Caches/pip/wheels/a6/ca/cc/884d54f268e802517e7ab6e625a6d597cfc3d132931d837230\n",
      "Successfully built env\n",
      "Installing collected packages: env\n",
      "Successfully installed env-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain neo4j openai tiktoken pytube youtube_transcript_api env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca051a-81ea-4245-badb-8fb4f97b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pytube import Playlist\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0125-e8e4-453a-84bb-2121d7463e65",
   "metadata": {},
   "source": [
    "## Using LangChain in combination with Neo4j to process YouTube playlists and perform Q&A flow\n",
    "\n",
    "### Motivation\n",
    "In a world of lengthy YouTube playlists, traditional learning can feel time-consuming and dull. Our motivation is to transform this process by making it dynamic and engaging. Rather than passively consuming content, I believe that sparking conversations can make learning more enjoyable and efficient.\n",
    "\n",
    "### Goal\n",
    "Our goal is to revolutionize how people interact with YouTube playlists. Users will actively engage in dynamic conversations inspired by the playlist content. We'll extract valuable information from video captions, process it, and integrate it into the Neo4j vector database. The conversational chain serving as a guide that leads users through an dialogue rooted in playlist content. My mission is to provide an interactive and personalized educational dialoge, where users actively shape their learning journey.\n",
    "\n",
    "#### Technologies used\n",
    "Embarking on the exciting journey of conversational AI requires a firm grasp of the technological foundations, that meet the needs of our mission. For our purpose we use synergy of two cutting-edge technologies: LangChain, an open-source framework simplifying the orchestration of Large Language Models (LLMs), and Neo4j, a robust graph database made for optimal node and relationship traversal.\n",
    "\n",
    "LangChain, serving as the linchpin in our quest for seamless interaction with LLMs. Its open-source nature enables developers to easily create and use the capabilities of these expansive language models. In our demo application, LangChain acts as the provider of an interface and construction of a conversational chain.\n",
    "\n",
    "At the heart of this interaction lies Neo4j, a graph database meticulously designed to unravel the complexities of interconnected nodes and relationships. This database isn't used just for storage; it's a dynamic source of truth that we integrate into our conversational framework.\n",
    "\n",
    "Picture this: a user initiates the conversation with a query, setting in motion a captivating exchange with our Large Language Model. The magic happens as the vector representation of the user's input becomes a beacon for exploration within the Neo4j graph database. The result? A seamless fusion of structured knowledge and natural language understanding, culminating in a response that is not just accurate but deeply connected to the context of the user's inquiry.\n",
    "\n",
    "Recognizing the importance of user experience, we introduce a conversational memory chain. Imagine a conversation where every question asked and every answer given becomes part of an evolving dialogue. This approach ensures that the interaction remains clear and coherent. By feeding all past questions and answers into the conversational memory chain alongside the latest query, we create a continuous narrative thread. The result? A more engaging, relevant, and user-centric conversation that evolves intelligently with each interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645380a3-77c9-46df-bb64-cedaa16b9503",
   "metadata": {},
   "source": [
    "### What will I cover in this tutorial\n",
    "1. Processing of YouTube playlists; reading captions\n",
    "2. Splitting each video captions into documents\n",
    "3. Feeding documents into Neo4j database\n",
    "4. Constructing conversational retrieval chain\n",
    "5. Performing queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df455324-5fb4-4e86-9391-1bbd308bb10c",
   "metadata": {},
   "source": [
    "### Processing of YouTube playlists\n",
    "Use `Playlist` package to retrieve all video IDs that are inside the given playlist. For every video, using `YouTubeLoader`, extract caption documents. Feed each document into text splitter. It is important to clear and preprocess the data before feeding it into text splitters. In our case, we ensured that we only considered English captions. The size of each chunk varies and should be set based on the nature of the documents. Smaller chunks, up to 256 tokens, capture information more granularly. Larger chunks provide our LLM with more context based on the information within each document. In our case, I decided to use a chunk size of 512. This decision was made because context is more imporant so we ensure contextual connection over multiple videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad79d5-471d-4b2d-94dd-16bae0323ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos from the playlist\n",
    "playlist_url = \"https://www.youtube.com/watch?v=1CqZo7nP8yQ&list=PL9Hl4pk2FsvUu4hzyhWed8Avu5nSUXYrb\"\n",
    "playlist = Playlist(playlist_url)\n",
    "video_ids = [_v.split('v=')[-1] for _v in playlist.video_urls]\n",
    "print(f\"Processing {len(videos)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ebe4f-fbce-451b-ac7a-515e566f8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup username, passwords and api keys\n",
    "from env import setup_env\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f35ac-3381-4634-a93c-1e6e151cab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read their captions and process it into documents with above defined text splitter\n",
    "documents = []\n",
    "for video_id in video_ids:\n",
    "    try:\n",
    "      loader = YoutubeLoader(video_id=video_id)\n",
    "      documents.append(loader.load()[0])\n",
    "    except: # if there are no english captions\n",
    "      pass\n",
    "print(f\"Read captions for {len(documents)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c02c4b-ce03-4bb1-8f54-befc15a42570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text splitter with chunk size 512 (https://www.pinecone.io/learn/chunking-strategies/)\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=20)\n",
    "# Split documents\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "print(f\"{len(splitted_documents} documents ready to be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddca89-b674-4098-878f-1f38735ddbb0",
   "metadata": {},
   "source": [
    "### Feeding documents into Neo4j database\n",
    "As mentioned earlier, all the documents will be stored inside the Neo4j database. In return, we will obtain a vector index that will later be utilized in conjunction with LangChain. Creating a Neo4j database is fairly straightforward and can be done without any additional knowledge of how the database operates and functions. Since we have already prepared all our documents and split them, we used the `from_documents` function, which accepts a `List[Document]`. To simplify this process even further, we could also use the `from_texts` function. However, in this case, we would lose control over documents. Therefore, I believe that `from_texts` should only be used when we quickly want to demonstrate an application. \n",
    "\n",
    "Setting `search_type` to `hybrid` will allow us to search over keywords and vectors. Hybrid search combines results from both [full text search](https://neo4j.com/docs/cypher-manual/current/indexes-for-full-text-search/) and vector queries which use different functions such as [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/). To merge the results, a Reciprocal Rank Fusion (RRF) algorithm is used. Response at the end provides only one result set, which is determined by RRF algorithm. This combination of vector search with traditional search methods allows for more nuanced and contextually relevant search results, improving the accuracy and depth of insights. This approach is particularly useful in applications such as ours where we have new answer every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59ea58-9acb-47c2-9585-a7becf25f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct vector\n",
    "neo4j_vector = Neo4jVector.from_documents(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    documents=splitted_documents,\n",
    "    url=os.environ['NEO4J_URI'],\n",
    "    username=os.environ['NEO4J_USERNAME'],\n",
    "    password=os.environ['NEO4J_PASSWORD'],\n",
    "    search_type=\"hybrid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816cdb1-2a5c-473a-b8ab-e8800cc59c10",
   "metadata": {},
   "source": [
    "![Graph1](youtube_playlist_1.png \"Graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb697b41-0b97-483d-86f8-b0858ccaef06",
   "metadata": {},
   "source": [
    "### Constructing conversational retrieval chain\n",
    "A conversational chain will be used to facilitate the Q&A flow. Setting `k` to `3` signals our retrieval chain to retain the last 3 messages in memory. These three messages will be passed to the LLM while performing queries. Adjusting this value will provide the LLM with more context during the Q&A flow. For retrieval, we will use the Neo4j vector instance generated earlier. Additionally, we set the maximum tokens (`max_tokens_limit`) to ensure that we stay within the specified limit.\n",
    "\n",
    "Understanding conversational retrieval chain is much easier if we split the process of asking a question and getting back the answer into three parts:\n",
    "1. Use the chat history and new question to create a \"standalone question\". This is done so that this question can be passed into the retrieval step to fetch relevant documents. If only the new question was passed in, then relevant context may be lacking. If the whole conversation was passed into retrieval, there may be unnecessary information there that would distract from retrieval.\n",
    "2. This new question is passed to the retriever and relevant documents are returned.\n",
    "3. The retrieved documents are passed to an LLM along with either the new question (default behavior) or the original question and chat history to generate a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab8d1d-063c-4485-b1ad-dbeae0aa8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Q&A object\n",
    "chat_mem_history = ChatMessageHistory(session_id=\"1\")\n",
    "mem = ConversationBufferWindowMemory(k=3, memory_key=\"chat_history\", chat_memory=chat_mem_history, return_messages=True)\n",
    "q = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.2),\n",
    "    memory=mem,\n",
    "    retriever=neo4j_vector.as_retriever(),\n",
    "    verbose=True,\n",
    "    max_tokens_limit=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7476-b4eb-4291-99db-0226444f417b",
   "metadata": {},
   "source": [
    "![Graph2](youtube_playlist_2.png \"Graph2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73bdf3-15d2-4fa0-9dff-8b85ba508cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Q&A flow - first question\n",
    "response = q.run('What can you tell me about the GenAI stack?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd345-a778-4196-99bf-641f8a87d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow up question that requires previous answers (memory)\n",
    "response = q.run('Who talked about it?')\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
