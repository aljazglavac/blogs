{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d43937-31c3-4815-845f-304b09e66e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain neo4j openai tiktoken pytube youtube_transcript_api env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca051a-81ea-4245-badb-8fb4f97b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pytube import Playlist\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b0125-e8e4-453a-84bb-2121d7463e65",
   "metadata": {},
   "source": [
    "## Using LangChain in combination with Neo4j to process YouTube playlists and perform Q&A flow\n",
    "\n",
    "### Motivation\n",
    "In a world of lengthy YouTube playlists, traditional learning can feel time-consuming and dull. Our motivation is to transform this process by making it dynamic and engaging. Rather than passively consuming content, I believe that sparking conversations can make learning more enjoyable and efficient.\n",
    "\n",
    "### Goal\n",
    "Our goal is to revolutionize how people interact with YouTube playlists. Users will actively engage in dynamic conversations inspired by the playlist content. We'll extract valuable information from video captions, process it, and integrate it into the Neo4j vector database. The conversational chain serving as a guide that leads users through an dialogue rooted in playlist content. My mission is to provide an interactive and personalized educational dialoge, where users actively shape their learning journey.\n",
    "\n",
    "#### Technologies used\n",
    "Embarking on the exciting journey of conversational AI requires a firm grasp of the technological foundations, that meet the needs of our mission. For our purpose we use synergy of two cutting-edge technologies: LangChain, an open-source framework simplifying the orchestration of Large Language Models (LLMs), and Neo4j, a robust graph database made for optimal node and relationship traversal.\n",
    "\n",
    "LangChain, serving as the linchpin in our quest for seamless interaction with LLMs. Its open-source nature enables developers to easily create and use the capabilities of these expansive language models. In our demo application, LangChain acts as the provider of an interface and construction of a conversational chain.\n",
    "\n",
    "At the heart of this interaction lies Neo4j, a graph database meticulously designed to unravel the complexities of interconnected nodes and relationships. This database isn't used just for storage; it's a dynamic source of truth that we integrate into our conversational framework.\n",
    "\n",
    "Picture this: a user initiates the conversation with a query, setting in motion a captivating exchange with our Large Language Model. The magic happens as the vector representation of the user's input becomes a beacon for exploration within the Neo4j graph database. The result? A seamless fusion of structured knowledge and natural language understanding, culminating in a response that is not just accurate but deeply connected to the context of the user's inquiry.\n",
    "\n",
    "Recognizing the importance of user experience, we introduce a conversational memory chain. Imagine a conversation where every question asked and every answer given becomes part of an evolving dialogue. This approach ensures that the interaction remains clear and coherent. By feeding all past questions and answers into the conversational memory chain alongside the latest query, we create a continuous narrative thread. The result? A more engaging, relevant, and user-centric conversation that evolves intelligently with each interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645380a3-77c9-46df-bb64-cedaa16b9503",
   "metadata": {},
   "source": [
    "### What will I cover in this tutorial\n",
    "1. Processing of YouTube playlists; reading captions\n",
    "2. Splitting each video captions into documents\n",
    "3. Feeding documents into Neo4j database\n",
    "4. Constructing conversational retrieval chain\n",
    "5. Performing queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df455324-5fb4-4e86-9391-1bbd308bb10c",
   "metadata": {},
   "source": [
    "### Processing of YouTube playlists\n",
    "Use `Playlist` package to retrieve all video IDs that are inside the given playlist. For every video, using `YouTubeLoader`, extract caption documents. Feed each document into text splitter. It is important to clear and preprocess the data before feeding it into text splitters. In our case, we ensured that we only considered English captions. The size of each chunk varies and should be set based on the nature of the documents. Smaller chunks, up to 256 tokens, capture information more granularly. Larger chunks provide our LLM with more context based on the information within each document. In our case, I decided to use a chunk size of 512. This decision was made because context is more imporant so we ensure contextual connection over multiple videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad79d5-471d-4b2d-94dd-16bae0323ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos from the playlist\n",
    "playlist_url = \"https://www.youtube.com/watch?v=1CqZo7nP8yQ&list=PL9Hl4pk2FsvUu4hzyhWed8Avu5nSUXYrb\"\n",
    "playlist = Playlist(playlist_url)\n",
    "video_ids = [_v.split('v=')[-1] for _v in playlist.video_urls]\n",
    "print(f\"Processing {len(videos)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ebe4f-fbce-451b-ac7a-515e566f8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup username, passwords and api keys\n",
    "# from env import setup_env\n",
    "# setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f35ac-3381-4634-a93c-1e6e151cab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read their captions and process it into documents with above defined text splitter\n",
    "documents = []\n",
    "for video_id in video_ids:\n",
    "    try:\n",
    "      loader = YoutubeLoader(video_id=video_id)\n",
    "      documents.append(loader.load()[0])\n",
    "    except: # if there are no english captions\n",
    "      pass\n",
    "print(f\"Read captions for {len(documents)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c02c4b-ce03-4bb1-8f54-befc15a42570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text splitter with chunk size 512 (https://www.pinecone.io/learn/chunking-strategies/)\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=20)\n",
    "# Split documents\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "print(f\"{len(splitted_documents} documents ready to be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddca89-b674-4098-878f-1f38735ddbb0",
   "metadata": {},
   "source": [
    "### Feeding documents into Neo4j database\n",
    "As mentioned earlier, all the documents will be stored inside the Neo4j database. In return, we will obtain a vector index that will later be utilized in conjunction with LangChain. Creating a Neo4j database is fairly straightforward and can be done without any additional knowledge of how the database operates and functions. Since we have already prepared all our documents and split them, we used the `from_documents` function, which accepts a `List[Document]`. To simplify this process even further, we could also use the `from_texts` function. However, in this case, we would lose control over documents. Therefore, I believe that `from_texts` should only be used when we quickly want to demonstrate an application. Setting `search_type` to `hybrid` will allow us to search over keywords and vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59ea58-9acb-47c2-9585-a7becf25f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct vector\n",
    "neo4j_vector = Neo4jVector.from_documents(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    documents=splitted_documents,\n",
    "    url=os.environ['NEO4J_URI'],\n",
    "    username=os.environ['NEO4J_USERNAME'],\n",
    "    password=os.environ['NEO4J_PASSWORD'],\n",
    "    search_type=\"hybrid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816cdb1-2a5c-473a-b8ab-e8800cc59c10",
   "metadata": {},
   "source": [
    "![Graph1](youtube_playlist_1.png \"Graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb697b41-0b97-483d-86f8-b0858ccaef06",
   "metadata": {},
   "source": [
    "### Constructing conversational retrieval chain\n",
    "Conversational chain will be used to perform Q&A flow, while leveraging previous user input and LLM outputs. We first construct the memory object. By setting `k` to `3`we are signaling our retrieval chain, to keep the last 3 messages inside the memory. This 3 messages will be passed to LLM, while preforming queries. Modifying this value will enable LLM to have more context during Q&A flow. As a retriever we will use Neo4j vector instance that we generated before. We are also setting max tokens (`max_tokens_limit`) to ensure that we stay below the limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab8d1d-063c-4485-b1ad-dbeae0aa8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Q&A object\n",
    "chat_mem_history = ChatMessageHistory(session_id=\"1\")\n",
    "mem = ConversationBufferWindowMemory(k=3, memory_key=\"chat_history\", chat_memory=chat_mem_history, return_messages=True)\n",
    "q = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.2),\n",
    "    memory=mem,\n",
    "    retriever=neo4j_vector.as_retriever(),\n",
    "    verbose=True,\n",
    "    max_tokens_limit=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7476-b4eb-4291-99db-0226444f417b",
   "metadata": {},
   "source": [
    "![Graph2](youtube_playlist_2.png \"Graph2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73bdf3-15d2-4fa0-9dff-8b85ba508cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Q&A flow - first question\n",
    "response = q.run('What can you tell me about the GenAI stack?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbd345-a778-4196-99bf-641f8a87d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow up question that requires previous answers (memory)\n",
    "response = q.run('Who talked about it?')\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
